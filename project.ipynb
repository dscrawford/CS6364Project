{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 5e-5\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# dir = '../input/riiid-test-answer-prediction/'\n",
    "dir = './'\n",
    "# Read large datasets: https://www.kaggle.com/rohanrao/tutorial-on-reading-large-datasets\n",
    "train_dtypes = {\n",
    "    \"row_id\": \"int64\",\n",
    "    \"timestamp\": \"int64\",\n",
    "    \"user_id\": \"int32\",\n",
    "    \"content_id\": \"int16\",\n",
    "    \"content_type_id\": \"boolean\",\n",
    "    \"task_container_id\": \"int16\",\n",
    "    \"user_answer\": \"int8\",\n",
    "    \"answered_correctly\": \"int8\",\n",
    "    \"prior_question_elapsed_time\": \"float32\",\n",
    "    \"prior_question_had_explanation\": \"boolean\"\n",
    "}\n",
    "\n",
    "# why we removed columns:\n",
    "#   row_id: redundant\n",
    "#   task_container_id: tells you what container this question is in\n",
    "#       the max container size is 5 so not really significant\n",
    "#   user_answer: doesnt really affect if the answer is correct\n",
    "#   prior_question_had_explanation: majority of learning (we assume) will be done from lectures, not answer explanations\n",
    "#\n",
    "\n",
    "req_cols = ['timestamp', 'user_id', 'content_id', 'content_type_id', 'answered_correctly']\n",
    "            # 'prior_question_elapsed_time']\n",
    "\n",
    "# function to convert the milliseconds to seconds at load time\n",
    "# messes with the dtypes above and doesn't really save time so it is commented (see converters)\n",
    "def mil_to_sec(val):\n",
    "    if val == '':\n",
    "        return np.NaN\n",
    "    return round(int(val)/1000)\n",
    "\n",
    "train_columns = pd.read_csv(dir + 'train.csv', usecols=req_cols, nrows=1).columns\n",
    "t_index = {col: i for (i, col) in enumerate(train_columns)}\n",
    "train = pd.read_csv(dir + 'train.csv', usecols=req_cols, dtype=train_dtypes,\n",
    "                       # converters={'timestamp': mil_to_sec,\n",
    "                       #             'prior_question_elapsed_time': mil_to_sec},\n",
    "                       nrows=100000).to_numpy()\n",
    "# the following lines are kinda cheating since we don't actually have all the data at once\n",
    "# train_df = raw_df[raw_df['content_type_id'] == 0]\n",
    "# lecture_events_df = raw_df[raw_df['content_type_id'] == 1]\n",
    "\n",
    "questions_df = pd.read_csv(dir + 'questions.csv')\n",
    "lectures_df = pd.read_csv(dir + 'lectures.csv')\n",
    "example_test_df = pd.read_csv(dir + 'example_test.csv')\n",
    "\n",
    "# train_df = train_df.drop(['content_id', 'content_type_id'], axis=1) # TODO: replace\n",
    "\n",
    "# we need a way to dummify the user id\n",
    "# pd.get_dummies(train_df, columns=['user_id']) # DO NOT TRY THIS IT WILL OVERLOAD RAM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "class StudentKnowledge:\n",
    "    def __init__(self, num_tags, question_avg_default=0.67):\n",
    "        self.lectures = []\n",
    "        self.lecture_timestamp = []\n",
    "        self.questions = []\n",
    "        self.question_timestamp = []\n",
    "        self.num_tags = num_tags\n",
    "        self.question_avg_default = question_avg_default\n",
    "\n",
    "    def add_lecture(self, lecture_tag, lecture_timestamp):\n",
    "        self.lectures.append(lecture_tag)\n",
    "        self.lecture_timestamp.append(lecture_timestamp)\n",
    "\n",
    "    def add_question(self, question_answered_right, question_timestamp):\n",
    "        self.questions.append(question_answered_right)\n",
    "        self.question_timestamp.append(question_timestamp)\n",
    "\n",
    "    def get_features(self, timestamp):\n",
    "        features = np.zeros(self.num_tags + 1)\n",
    "\n",
    "        before_ts_lecture = [lt > timestamp for lt in self.lecture_timestamp]\n",
    "        valid_tags_lecture = [self.lectures[i] for i in range(len(self.lectures)) if before_ts_lecture[i]]\n",
    "\n",
    "        before_ts_question = [qt > timestamp for qt in self.question_timestamp]\n",
    "        num_questions_answered = sum(before_ts_question)\n",
    "        if num_questions_answered == 0:\n",
    "            question_avg = self.question_avg_default\n",
    "        else:\n",
    "            question_avg = sum([self.questions[i] for i in range(len(self.questions)) if before_ts_question[i]]) / num_questions_answered\n",
    "        features[valid_tags_lecture] = 1\n",
    "        features[-1] = question_avg\n",
    "        return features\n",
    "\n",
    "class StudentDataset:\n",
    "    def __init__(self, lectures, questions, train_columns):\n",
    "        self.students = {}\n",
    "        self.lecture_tags = {row['lecture_id']: row['tag'] for _, row in lectures.iterrows()}\n",
    "        self.question_tags = {row['question_id']: [int(x) for x in str(row['tags']).split() if x != 'nan'] for _, row in questions.iterrows()}\n",
    "        self.t_index = {col: i for i, col in enumerate(train_columns)}\n",
    "\n",
    "        a = questions['tags'].apply(lambda r: [int(x) for x in str(r).split() if x != 'nan']).to_numpy()\n",
    "        self.num_tags = len(np.unique([x for b in a for x in b]))\n",
    "\n",
    "    def read(self, row):\n",
    "        student_id = row[self.t_index['user_id']]\n",
    "        content_type_id = row[self.t_index['content_type_id']]\n",
    "        content_id = row[self.t_index['content_id']]\n",
    "        timestamp = row[self.t_index['timestamp']]\n",
    "        answered_right = row[self.t_index['answered_correctly']]\n",
    "        self.read_info(student_id, content_type_id, content_id, timestamp, answered_right)\n",
    "\n",
    "    def read_info(self, student_id, content_type_id, content_id, timestamp, answered_right):\n",
    "        if student_id not in self.students:\n",
    "            self.students[student_id] = StudentKnowledge(self.num_tags)\n",
    "        if content_type_id == 0:\n",
    "            self.students[student_id].add_question(answered_right, timestamp)\n",
    "        else:\n",
    "            self.students[student_id].add_lecture(self.lecture_tags[content_id], timestamp)\n",
    "\n",
    "    # Must receive a question vector, lectures not valid\n",
    "    def get_features(self, rows):\n",
    "        user_ids = rows[:,self.t_index['user_id']]\n",
    "        timestamps = rows[:,self.t_index['timestamp']]\n",
    "        question_ids = rows[:, self.t_index['content_id']]\n",
    "\n",
    "        question_features = np.zeros((len(rows), self.num_tags))\n",
    "        for i, question_id in enumerate(question_ids):\n",
    "            question_features[i,self.question_tags[question_id]] = 1\n",
    "\n",
    "        student_features = np.array([self.students[user_id].get_features(timestamp)\n",
    "                                     for user_id, timestamp in zip(user_ids, timestamps)])\n",
    "\n",
    "        return np.hstack([question_features, student_features])\n",
    "\n",
    "student_dataset = StudentDataset(lectures_df, questions_df, train_columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:00<00:00, 831873.07it/s]\n"
     ]
    }
   ],
   "source": [
    "for row in tqdm(train):\n",
    "    student_dataset.read(row)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "  def __init__(self, inputs, hidden, lr=0.001):\n",
    "    super().__init__()\n",
    "    self.main = nn.Sequential(\n",
    "      nn.Linear(inputs, hidden),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden, 1),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.main(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# Select only the questions\n",
    "train = train[train[:,t_index['content_type_id']]==0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.6310918636181275: 100%|██████████| 192/192 [00:21<00:00,  8.75it/s]\n",
      "Avg Loss: 0.6108763433634835: 100%|██████████| 192/192 [00:21<00:00,  8.90it/s]\n",
      "Avg Loss: 0.6054975811616462: 100%|██████████| 192/192 [00:22<00:00,  8.59it/s]\n",
      "Avg Loss: 0.6007638281858066: 100%|██████████| 192/192 [00:22<00:00,  8.67it/s]\n",
      "Avg Loss: 0.5980724128813595: 100%|██████████| 192/192 [00:22<00:00,  8.68it/s]\n",
      "Avg Loss: 0.5960870792887074: 100%|██████████| 192/192 [00:21<00:00,  8.78it/s]\n",
      "Avg Loss: 0.5949339064802666: 100%|██████████| 192/192 [00:21<00:00,  8.94it/s]\n",
      "Avg Loss: 0.5943859391270646: 100%|██████████| 192/192 [00:21<00:00,  9.05it/s]\n",
      "Avg Loss: 0.5932434733518965: 100%|██████████| 192/192 [00:21<00:00,  8.78it/s]\n",
      "Avg Loss: 0.5922841803260835: 100%|██████████| 192/192 [00:22<00:00,  8.63it/s]\n",
      "Avg Loss: 0.5900551537679559: 100%|██████████| 192/192 [00:23<00:00,  8.12it/s]\n",
      "Avg Loss: 0.5891324962548479: 100%|██████████| 192/192 [00:23<00:00,  8.23it/s]\n",
      "Avg Loss: 0.5888445949064406: 100%|██████████| 192/192 [00:21<00:00,  8.75it/s]\n",
      "Avg Loss: 0.5873944874556867: 100%|██████████| 192/192 [00:21<00:00,  8.75it/s]\n",
      "Avg Loss: 0.5867718174813495: 100%|██████████| 192/192 [00:22<00:00,  8.50it/s]\n",
      "Avg Loss: 0.5876040521281621: 100%|██████████| 192/192 [00:22<00:00,  8.69it/s]\n",
      "Avg Loss: 0.5869914113774676: 100%|██████████| 192/192 [00:21<00:00,  8.73it/s]\n",
      "Avg Loss: 0.5887025106567828: 100%|██████████| 192/192 [00:23<00:00,  8.21it/s]\n",
      "Avg Loss: 0.586966537848403: 100%|██████████| 192/192 [00:23<00:00,  8.11it/s] \n",
      "Avg Loss: 0.5852811057307619: 100%|██████████| 192/192 [00:23<00:00,  8.23it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Network(len(student_dataset.get_features(train[0:10])[0]), 16).to(device)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "for epoch in range(20):\n",
    "    batch_iter = list(range(0, len(train), BATCH_SIZE)) + [len(train)]\n",
    "    tq = tqdm(range(len(batch_iter) - 1))\n",
    "\n",
    "    # Average Loss Variables\n",
    "    total_loss = 0\n",
    "    current_len = 0\n",
    "\n",
    "    for i in tq:\n",
    "        batch = torch.tensor(\n",
    "            student_dataset.get_features(np.array(train[batch_iter[i] : batch_iter[i+1]]))\n",
    "        ).float().to(device)\n",
    "\n",
    "        labels = torch.tensor(\n",
    "            train[batch_iter[i] : batch_iter[i + 1], t_index['answered_correctly']].astype(np.bool)\n",
    "        ).view(len(batch), 1).float().to(device)\n",
    "\n",
    "        predictions = model(batch)\n",
    "\n",
    "        loss = loss_func(predictions, labels.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_len += len(batch)\n",
    "        total_loss += loss.item() * len(batch)\n",
    "\n",
    "        tq.set_description('Avg Loss: ' + str(total_loss / current_len))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7025625878470595\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_correct = 0\n",
    "    for row in train: # replace with test/val set\n",
    "        prediction = model(torch.tensor(student_dataset.get_features(np.array([row[:-1]]))).float().to(device))\n",
    "        total_correct += round(torch.sigmoid(prediction).item()) == row[-1]\n",
    "    print(total_correct/len(train))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.47721028327941895: 100%|██████████| 982/982 [00:01<00:00, 646.16it/s]\n",
      "0.5117090940475464: 100%|██████████| 982/982 [00:01<00:00, 694.01it/s] \n",
      "0.5589438676834106: 100%|██████████| 982/982 [00:01<00:00, 691.92it/s] \n",
      "0.6039080619812012: 100%|██████████| 982/982 [00:01<00:00, 714.20it/s] \n",
      "0.6484382748603821: 100%|██████████| 982/982 [00:01<00:00, 710.09it/s] \n",
      "0.675808846950531: 100%|██████████| 982/982 [00:01<00:00, 686.56it/s]   \n",
      "0.71211838722229: 100%|██████████| 982/982 [00:01<00:00, 673.43it/s]    \n",
      "0.754368782043457: 100%|██████████| 982/982 [00:01<00:00, 646.40it/s]   \n",
      "0.7781763672828674: 100%|██████████| 982/982 [00:01<00:00, 652.69it/s]  \n",
      "0.801904022693634: 100%|██████████| 982/982 [00:01<00:00, 652.96it/s]   \n",
      "0.8250249028205872: 100%|██████████| 982/982 [00:01<00:00, 656.88it/s]  \n",
      "0.8431645035743713: 100%|██████████| 982/982 [00:01<00:00, 577.36it/s]   \n",
      "0.8558878302574158: 100%|██████████| 982/982 [00:01<00:00, 657.00it/s]   \n",
      "0.8667155504226685: 100%|██████████| 982/982 [00:01<00:00, 711.46it/s]   \n",
      "0.8806516528129578: 100%|██████████| 982/982 [00:01<00:00, 717.19it/s]   \n",
      "0.8886072039604187: 100%|██████████| 982/982 [00:01<00:00, 708.89it/s]   \n",
      "0.8921254873275757: 100%|██████████| 982/982 [00:01<00:00, 707.64it/s]    \n",
      "0.9092730283737183: 100%|██████████| 982/982 [00:01<00:00, 718.33it/s]    \n",
      "0.900749146938324: 100%|██████████| 982/982 [00:01<00:00, 704.94it/s]     \n",
      "0.9233396053314209: 100%|██████████| 982/982 [00:01<00:00, 706.36it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 0.6595275169295352\n",
      "mean loss: 0.6426682004708865\n",
      "mean loss: 0.6291753450473069\n",
      "mean loss: 0.6133536751364982\n",
      "mean loss: 0.5950161170578597\n",
      "mean loss: 0.5767507815767694\n",
      "mean loss: 0.558864320958386\n",
      "mean loss: 0.5422472487528127\n",
      "mean loss: 0.5268002794804184\n",
      "mean loss: 0.5129485195836948\n",
      "mean loss: 0.49938805383335383\n",
      "mean loss: 0.4870936342406371\n",
      "mean loss: 0.4750490730202394\n",
      "mean loss: 0.4639553778206947\n",
      "mean loss: 0.45314748302022134\n",
      "mean loss: 0.44310402308372693\n",
      "mean loss: 0.43323565440822187\n",
      "mean loss: 0.4242707047260686\n",
      "mean loss: 0.41552238765069466\n",
      "mean loss: 0.40741920772696494\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_correct = 0\n",
    "    for row in train: # replace with test/val set\n",
    "        prediction = model(torch.tensor(student_dataset.get_features(np.array([row[:-1]]))).float())\n",
    "        total_correct += round(torch.sigmoid(prediction).item()) == row[-1]\n",
    "    print(total_correct/len(train))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8319755600814664\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_correct = 0\n",
    "    for row in train: # replace with test/val set\n",
    "        prediction = model(torch.tensor(student_dataset.get_features(np.array([row[:-1]]))).float())\n",
    "        total_correct += round(torch.sigmoid(prediction).item()) == row[-1]\n",
    "    print(total_correct/len(train))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}